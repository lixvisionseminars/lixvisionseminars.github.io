- title: "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time"
  authors: <a href="#">Guillaume Le Moing</a>
  date: 26-02-2026
  year: 2026
  thumbnail: /assets/talks/dart.png
  pdf: https://arxiv.org/abs/2512.08924
  website: https://d4rt-paper.github.io
  abstract: "Traditional approaches to 4D scene reconstruction often rely on computationally heavy iterative optimization or fragmented pipelines, piecing together separate models for depth, motion, and camera estimation. In this talk, we introduce D4RT (Dynamic 4D Reconstruction and Tracking), a powerful feedforward model that unifies these tasks into a single, scalable architecture. We will explore D4RTâ€™s core innovation: a lightweight decoding mechanism paired with a robust video encoder, allowing it to independently query the 3D position of any pixel across space and time. We will also discuss the key design choices that enable D4RT to achieve state-of-the-art performance across reconstruction, tracking, and camera benchmarks. Finally, we will showcase its capabilities through qualitative results and baseline comparisons, before concluding with a discussion of current limitations and future directions."
  register: https://docs.google.com/spreadsheets/d/1LesLZnkBfh5GFv5OZV5VAeb7FbMewthCtPuE_sb4Zms/edit?gid=0#gid=0
  bio: "Guillaume Le Moing is a Research Scientist at Google DeepMind in Paris. He completed his PhD at Willow (Inria Paris) with Cordelia Schmid and Jean Ponce where his research focused on future video prediction (CCVS, WALDO) and motion understanding (DOT). Since joining DeepMind, he has worked on video foundation models (4DS & SciVid) and 4D vision (D4RT)."
  location: Grace Hopper Room, 2:00 PM